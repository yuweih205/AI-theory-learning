他在prompt里面加入了输入格式什么什么的 而不是直接训练出来的

训练过程

感觉写的不是很清楚，也可能是我没读懂。

* zero是纯GRPO
  *  为了节约计算成本(估计加上critic model也没啥太大的效果)，只有policy model，什么是policy，数学原理上我还不太明白，但是强化学习里面每走一步他会得到一个reward值，policy model负责告诉他下一步这个值reward的期望值是多少，G是group，意思是，每次输入，会生成一组候选，这组候选的平均值就是baseline，这样就可以知道哪个比这个base好，以此来更新策略，~~reward model也可以用v3来充当~~，这样可以学到各种回答的好坏直接**程度**
  * 其实最重要的就是reward model，采用了规则制定，两方面，一个是准确度，二是格式，不采取神经网络反馈模型是因为可能在RL里面会遇到reward hacking（只在RL里面有吗 啥时候会触发该bug），而且重新训练需要很多计算资源，让pipeline变得非常复杂
* R1是sft+GPRO 
* 蒸馏也只做了sft，能力有所增强，据说加上强化学习会更强