* AGI 通用人工智能 类人 不仅仅说话等等
* softmax操作天然会产生稀疏分布 ？？
* liner attenton
  * 先做Kt x V  线性复杂度
* 如何分块 
* route strategy

* 这个既然类似moe   只选择特定的模块 

finer grain更精细的粒度

* 就是说moba是通过每层都可以选择全注意力或者分块滑窗 他还通过moe选择更远的块 这个moe怎么训练出来也很无解

这玩意怎么训练 怎么梯度回传啊

长序列 上下文窗口大小 每个训练样本中所能看到的最大输入序列长度  一次前向传播中能处理的token序列长度 这个和梯度累积有没有什么关系

* postion scaling是怎么做的 position interpolation是怎么做的
* 第9页这些数据的比较指的是什么 带训练的 还是纯推理 感觉应该是纯推理的 再确认下
* Needle in the Haystack 这是一种测试方法 在超长上下文中 插入一段重要信息 看模型是否能提取出来
* 语言模型的loss和真实世界任务表现
  * 语言模型loss 交叉熵损失 用来衡量预测token概率和真实偏差 通用预料、书籍、wikipedia 真实世界任务表现是真实任务的准确率 有点像 语言模型强调上下文的预测性 真实世界任务强调能实际解决问题
* 只对key侧进行块级选择 